---
title: "K-Nearest Neighbours"
format: 
  html:
    toc: true
editor_options: 
  chunk_output_type: console
---



## Introduction
- A simple algorithm in which each observation is predicted based on "its similarity" to other observations.
- KNN is a *memory-based* algorithm. *The training samples are required at run-time and predictions are made directly from sample relationships.* 
  - Memory based algorithm is a type of non-parametric algorithm.
  - These algorithms search for the training data that are most similar to the test data and make predictions based on similarities.
  - Memory based algorithms are mostly used for *supervised learning* only.
  - These algorithms are computationally costly, especially if the training set is too large, as the storage of the entire data set is required.
  - In case of a *model-based* learning algorithm, after building the model to predict the targets, the training set are discarded, so there is no need to store them. 
- KNNs are also known as *lazy learners* and can be computationally inefficient.
  - Lazy learner algorithms simply store the data and *generalizing beyond this data* is postponed until an explicit request is made.
  - In contrast, the *eager-learning* algorithms construct general, explicit description of the target function based on the provided training examples.
  
## Measuring similarity
- KNN algorithm identifies *K* observations that are similar or nearest to the new record being predicted.
- Then it uses the *average response value* (regression) or the *most common class* (classification) of those *K* observations as the predicted output.
- For example consider the Ames housing data.
  - In real estate, the realtors list the price of a house based on the similar characteristics (e.g. area, number of rooms, location, etc.) of other homes.
  - This is what the KNN algorithm will do.
  
![The 10 nearest neighbors (blue) whose home attributes most closely resemble the house of interest (red).](https://bradleyboehmke.github.io/HOML/06c-knn_files/figure-html/map-homes-1.png)

### Distance measures
- How do we determine the similarity between observations? 
- We use distance (or dissimilarity) metrics to compute the pairwise differences between observations. 
- The most common distance measures are the *Euclidean* and *Manhattan* distance metrics; both of which measure the distance between observation $x_a$ and $x_b$ for all $j$ features.
$$
\begin{align*}
\text{Euclidean Distance} &= \sqrt{\sum_{j=1}^P(x_{aj} - x_{bj})^2} \\\
\text{Manhattan Distance} &= \sum_{j=1}^P|x_{aj} - x_{bj}|
\end{align*}
$$
- Let's calculate the two distances and plot them using the `AmesHousing` package. 
- The data we shall use should be processed and cleaned using the `make_ames()` function. 

```{r}
library(AmesHousing)
(ames <- make_ames())
```

- Let us compute the distances for the first two homes and two features/variables `(Gr_Liv_Area & Year_Built)` in the dataset.

```{r}
#| warning: false
#| message: false
library(tidyverse)
ames %>% 
  select(Gr_Liv_Area, Year_Built) %>% 
  slice(1:2) -> TwoHouses

TwoHouses %>% 
  dist(method = "euclidean")

TwoHouses %>% 
  dist(method = "manhattan")
```

```{r}
#| code-fold: true
#| message: false
#| warning: false
gridExtra::grid.arrange(
TwoHouses %>% 
  ggplot(aes(Gr_Liv_Area, Year_Built))+
  geom_point()+
  geom_line(lty = "dashed")+
  labs(title = "Euclidean Distance"),

TwoHouses %>% 
  ggplot(aes(Gr_Liv_Area, Year_Built))+
  geom_point()+
  geom_step(lty = "dashed")+
  labs(title = "Manhattan Distance"),
nrow = 1)
```

### Pre-processing
- Data preprocessing makes up a major portion of any project involving data, and data scientists devote around 80% of their time on gathering and organising data.
- Analyzing, filtering, manipulating, and encoding data is known as "data preprocessing," and it is done so that a machine learning algorithm can comprehend and use the processed output.
- The Euclidean distance is more sensitive to outliers because of the squared term in the formula.









---
title: "Classification"
format: 
  revealjs:
    scrollable: true
    code-fold: true
    echo: true
    eval: true
editor_options: 
  chunk_output_type: console
---

## Classifying based on similarities with *k*-nearest neighbours.

- The *k*-nearest neighbour algorithm uses labeled data and therefore it is a supervised learning algorithm.
- How does the kNN learn?
  - Examples of snakes in UK 
  - Two species of venomous snakes- grass snake and adder.
  - Slow worm - a limbless reptile mistaken for a snake.
- Objective: Build a kNN classifier to help you quickly classify future specimens you come across.

---

- **Figure 3.1** Body length and aggression of reptiles. 

![](5.Classification_files/fig3.1.png)


---

- The *two* phases of the kNN algorithm;
  1. The training phase: In this phase the algorithm stores the data.
  2. The prediction phase: In this phase the kNN algorithm calculates the distance between each new, unlabeled case and all the labeled cases. *The term <u> distance </u> indicates the nearness in terms of body-weight and aggression.*
- The distance metric is often called the **Euclidean distance** (a straight-line distance between two points on a plot).

---

- **Figure 3.2** The first step of kNN algorithm: calculating distance.

![](5.Classification_files/fig3.2.png)

---

- Next, for each unlabeled case, the algorithm ranks the neighbours from the nearest (most similar) to the furthest (the least similar).
- **Figure 3.3** The second step of the kNN algorithm: ranking the neighbours.

![](5.Classification_files/fig3.3.png)


---

- The algorithm identifies *k*-labelled cases nearest to unlabeled cases.
- The *k*-labeled cases are most similar in-terms of the variables to the unlabeled case.
- Whatever class most of the *k*-nearest neighbours belong to is what the unlabeled case is classified as.

---

- **Figure 3.4** The final step: Identifying the *k*-nearest neighbours and taking the majority vote.

![](5.Classification_files/fig3.4.JPG)


## Building your first kNN model.
- You are trying to improve the diagnosis of patients with diabetes.
- You collect data and record if they were diagnosed as *healthy*, *chemically diabetic* or *overtly diabetic*.
- You want to use a kNN algorithm to train a model that can predict which of these classes a new patient will belong to so that diagnoses can be improved.
- This is a **three class classification** problem.

---

- Lets look at the diabetes data
```{r}
# required packages
library(mclust)
library(tidyverse)
library(mlr)

# the dataset
data(diabetes, package = 'mclust')
(as_tibble(diabetes) -> diabetes)
```

- The `class` shows three cases of diabetes - Normal, Chemical, and Overt.
- `glucose`: the level of blood glucose.
- `insulin`: the level of insulin 
- `sspg`: steady state level of blood glucose.

---



---
title: "Modeling process"
author: "Pravesh"
date: "September 2021"
output: 
  ioslides_presentation:
    smaller: true
    logo: logo.png
    css: mycss.css
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 150px;
}
```

```{r, echo=FALSE}
options(scipen = 999, digits = 4)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

```

## Reference
- [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/) by Bradley Boehmke & Brandon Greenwell


## Introduction

- The ML process is iterative and heuristic based approach, just like EDA.
- Without the knowledge about the data or the problem at hand it is really difficult to ascertain which ML will perform the best. This is known as the *no free lunch* theorem for ML.
- ML approaches need to be applied, evaluated, and modified before a final model is determined.
- Approaching ML modelling correctly means
  - Spending our data wisely on *learning and validation*
  - Properly *pre-processing* the feature and the target variables,
  - Minimizing *data leakage*
  - Tuning *hyperparameters*, and 
  - Assessing model performance

---

![](modellingProcess/fig2.1.png){width=90%}

## What are *hyperparameters* in ML?
- Hyperparameters are the parameters that are explicitly defined by the user to control the learning process.
- The value(s) of the hyperparameter are set by the ML engineer before the learning algorithm begins training the model.
- These values are external and cannot be changed during the trainig process.
- Some examples of hyperparameters
  - the *k* in kNN
  - Train-test split ratio
  - branches in decision tree
  - number of clusters in clustering algorithm
  
## Difference between a *hyperparameter* and a *model parameter*.
- Model parameters are internal to the model, and a model learns them on its own.
- Example: coefficients of independent variables in Linear Regression Model.


## Data sets
```{r}
library(tidyverse)
```


- Property sales information as described in De Cock (2011).
  - problem type: supervised regression
  - response variable: Sale_Price (i.e., $195,000, $215,000)
  - features: 80
  - observations: 2,930
  - objective: use property attributes to predict the sale price of a home
```{r, message=FALSE, warning=FALSE}
## install.packages("AmesHousing", dependencies = TRUE)
## ?AmesHousing::make_ames()
ames <- AmesHousing::make_ames()
```


---


- Employee attrition information originally provided by IBM Watson Analytics Lab.
  - problem type: supervised binomial classification
  - response variable: Attrition (i.e., “Yes”, “No”)
  - features: 30
  - observations: 1,470
  - objective: use employee attributes to predict if they will attrit (leave the company)
```{r, message=FALSE, warning=FALSE}
## install.packages("modeldata")
data(attrition, package = "modeldata")
attrition <- as_tibble(attrition)
```


# Data splitting

---


- Goal of ML: Find an algorithm $f(X)$ that most accurately predicts future values $\hat{Y}$ based on a set of features $X$.
- **Generalisability** of an algorithm:
  - We want an algorithm that not only fits well to our past data, but also predicts the future outcome accurately.
- To provide an understanding of the generalisability of our model we split our data to training and test data sets.
- **Training set**: Data used to develop feature sets, train our algorithm, tune hyperparameters, compare models, and all other activities required to choose a final model.
- **Test set**: having chosen a final model, this data set is used to estimate an unbiased assessment of the model's performance, which is referred to as the generalisation error.
- Test set should not be used before the selecting the final model.


---

- Recommendation for data splitting; 60-40, 70-30, or 80-20.
- Spending too much in *training* (>80) wont allow us a good assessment of predictive performance.
  - The model might fit the training data very well, but is not generalisable (*overfitting*).
- Spending too much on *testing* won't allow to get a good assessment of model parameters.
- Two common ways of splitting data:
  - Simple random sampling
  - Stratified sampling
  
## Simple random sampling
```{r}
# using the rsample package
library(rsample)
set.seed(123)
split_1 <- initial_split(ames, prop = 0.7)
train_1 <- training(split_1)
test_1 <- testing(split_1)
```

## Stratified sampling
- The response variable may be severely imbalanced.
  - 90% observations with "Yes" responses and 10% with "No" responses.
  - A continuous response variable which is positively or negatively skewed.
    - Segment the response variables into quantiles and randomly sample from each.
- In the `attrition` dataset, the variable `Attrition` has imbalanced responses  
```{r}
table(attrition$Attrition) %>% 
  prop.table()
```

---

- Stratified sampling with `rsample`
```{r}
set.seed(123)
split_strata <- initial_split(attrition, prop = 0.7, strata = "Attrition")
train_strata <- training(split_strata)
test_strata <- testing(split_strata)
```

- Check the proportion of "Yes" and "No" responses in the training and the testing data.
```{r}
train_strata$Attrition %>% table() %>% prop.table()
test_strata$Attrition %>% table() %>% prop.table()
```

## Class imbalances
- Imbalanced data can impact model predictions and performance.
- Mostly seen in *classification* problems
  - 5% defaults vs 95% non-defaults
- Categories of sampling methods to remedy class imbalances
  <ol>
  <li> Up-sampling
  </ol>
    - Quantity of data is not sufficient
    - Balance the data by increasing the sample of the rarer samples.
    - Samples generated by repetition or bootstrapping.


  <ol start=2> 
  <li> Down-sampling
  </ol>
    - Reduce the size of the abundant class
    - Quantity of data is sufficient
    - Keep all the samples in the rare class and randomly select an equal size of samples in the abundant class.
    
  
## Creating models in R

# Resampling methods

---

- Split the data into *training* and *testing* sets.
- The *test* set cannot be used to assess the model performance.
- How to assess the generalisation performance of the model?
- *Validation approach*
  - Split the training set to further two parts
    - Training set
    - Validation or holdout set
  - Train the model on the new training set and estimate the performance on the validation set.
- **Resampling methods** repeatedly fit a model of interest to parts of the training data and test its performance on other parts.
  1. *k-fold cross validation*
  2. *bootstrapping*

## *k*-fold cross validation
- Randomly divides the *training* data into k-groups (aka folds) of approximately equal size
- The model is fit on *k-1* folds and the remaining fold is used to compute model performance.
- The procedure is repeated *k* times, each time a different fold is treated as the validation set.
- This results in *k* estimates of the generalisation error
- The average of these *k* estimates gives us the *k*-fold CV estimate (i.e. an approximation of the error that we might see on unseen data).

---

![Illustration of *k*-fold cross validation](https://bradleyboehmke.github.io/HOML/images/cv.png){width=700px}

---

- In practice one typically uses *k=5* or *k=10*
- There is no formal rule for the size of *k*.
- As *k* gets larger, the difference between the estimated performance and true performance to be seen on the test set will decrease.
- On the other hand, using too large *k* can introduce computational burdens.
- For smaller data sets (say *n<10,000*), 10-fold CV repeated 5 or 10 times will improve the accuracy of your estimated performance and also provide an estimate of its variability.


---

![10-fold cross validation on 32 observations. Each observation is used once for validation and nine times for training. ](https://bradleyboehmke.github.io/HOML/02-modeling-process_files/figure-html/modeling-process-cv-1.png){width=700px}

## Bootstrapping
- A bootstrap sample is a random sample of the data taken with replacement (Efron and Tibshirani 1986).
- A bootstrap sample is the same size as the original data set from which it was constructed. 
- bootstrap sampling will contain approximately the same distribution of values.
![](https://bradleyboehmke.github.io/HOML/images/bootstrap-scheme.png){width=700px}

---

- Each bootstrap is likely to contain duplicate values
  - About 63.21% of original values are found in bootstrap sample.
- The observations not contained in bootstrap samples are considered *out-of-bag* (OOB).
- When bootstrapping, a model can be built on the selected samples and validated on the OOB samples; this is often done, for example, in random forests.
- Observations are replicated in bootstrapping, 
  - less variability in the error measure compared with *k*-fold CV
  - Increase the bias of the error estimate.
  - Not an issue with data sets ($n \geq 1000$).
  
## Bias
- Difference between the expected/average prediction and the correct value which we are trying to predict is known as the **bias**.
- If a model has a high bias, it will have consistency in its resampling performance as illustrated in the figure (right).

![A biased polynomial model fit to a single data set does not capture the underlying non-linear, non-monotonic data structure (left). Models fit to 25 bootstrapped replicates of the data are underterred by the noise and generates similar, yet still biased, predictions (right).](https://bradleyboehmke.github.io/HOML/02-modeling-process_files/figure-html/modeling-process-bias-model-1.png){width=700px}


## Variance
- Error due to **variance** is defined as the variability of the model prediction for a given data point.
![A high variance k-nearest neighbor model fit to a single data set captures the underlying non-linear, non-monotonic data structure well but also overfits to individual data points (left). Models fit to 25 bootstrapped replicates of the data are deterred by the noise and generate highly variable predictions (right). ](https://bradleyboehmke.github.io/HOML/02-modeling-process_files/figure-html/modeling-process-variance-model-1.png){width=700px}

---

- Since high variance models are more prone to overfitting, using resampling procedures are critical to reduce this risk. - Overfitting: Although you may achieve very good performance on your training data, the model will not automatically generalize well to unseen data.


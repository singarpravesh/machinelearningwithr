---
title: "Classification"
format: 
  revealjs:
    scrollable: true
    code-fold: true
    echo: true
    eval: true
editor_options: 
  chunk_output_type: console
---

## Classifying based on similarities with *k*-nearest neighbours.

- The *k*-nearest neighbour algorithm uses labeled data and therefore it is a supervised learning algorithm.
- How does the kNN learn?
  - Examples of snakes in UK 
  - Two species of venomous snakes- grass snake and adder.
  - Slow worm - a limbless reptile mistaken for a snake.
- Objective: Build a kNN classifier to help you quickly classify future specimens you come across.

---

- **Figure 3.1** Body length and aggression of reptiles. 

![](5.Classification_files/fig3.1.png)


---

- The *two* phases of the kNN algorithm;
  1. The training phase: In this phase the algorithm stores the data.
  2. The prediction phase: In this phase the kNN algorithm calculates the distance between each new, unlabeled case and all the labeled cases. *The term <u> distance </u> indicates the nearness in terms of body-weight and aggression.*
- The distance metric is often called the **Euclidean distance** (a straight-line distance between two points on a plot).

---

- **Figure 3.2** The first step of kNN algorithm: calculating distance.

![](5.Classification_files/fig3.2.png)

---

- Next, for each unlabeled case, the algorithm ranks the neighbours from the nearest (most similar) to the furthest (the least similar).
- **Figure 3.3** The second step of the kNN algorithm: ranking the neighbours.

![](5.Classification_files/fig3.3.png)


---

- The algorithm identifies *k*-labelled cases nearest to unlabeled cases.
- The *k*-labeled cases are most similar in-terms of the variables to the unlabeled case.
- Whatever class most of the *k*-nearest neighbours belong to is what the unlabeled case is classified as.

---

- **Figure 3.4** The final step: Identifying the *k*-nearest neighbours and taking the majority vote.

![](5.Classification_files/fig3.4.JPG)


## Building your first kNN model.
- You are trying to improve the diagnosis of patients with diabetes.
- You collect data and record if they were diagnosed as *healthy*, *chemically diabetic* or *overtly diabetic*.
- You want to use a kNN algorithm to train a model that can predict which of these classes a new patient will belong to so that diagnoses can be improved.
- This is a **three class classification** problem.

---

- Lets look at the diabetes data
```{r}
# required packages
library(mclust)
library(tidyverse)
library(mlr)

# the dataset
data(diabetes, package = 'mclust')
(as_tibble(diabetes) -> diabetes)
```

- The `class` shows three cases of diabetes - Normal, Chemical, and Overt.
- `glucose`: the level of blood glucose.
- `insulin`: the level of insulin 
- `sspg`: steady state level of blood glucose.

---

- The relationship between the variables are plotted below:


```{r}
diabetes %>% 
  ggplot(aes(glucose, insulin))+
  geom_point(aes(col = class))
```


```{r}
diabetes %>% 
  ggplot(aes(sspg, insulin))+
  geom_point(aes(col = class))
```

```{r}
diabetes %>% 
  ggplot(aes(sspg, glucose))+
  geom_point(aes(col = class))
```

---

**Exercise 1**

Reproduce the plot of glucose versus insulin, but use shapes rather than colors to indicate which class each case belongs to. Once youâ€™ve done this, modify your code to represent the classes using shape and color.

---

- From the data (scatter plots too) we can see that there are differences in the continuous variables among the three classes.
- So let's build a kNN classifier that can **predict diabetes status from measurements of future patients**.
- It is important to **scale** the predictor variables by dividing them by their standard deviation. 
  - This preserves the relationship between the variables.
  - This ensures that variables measured on larger scales are not given more importance by the algorithm.
- In our case, the `mlr` package can take care of the **scaling** of the varaibles as it is in the algorithm.


## Using `mlr` to train the kNN model
- The three stages in `mlr` package:
  1. **Define the task**: Classify the data with the `class` variable as the target variable.
  2. **Define the learner**: The learner is the name of the algorithm that we want to use.
  3. **Train the model**: After the learner generates the model, use it to make future predictions.
  
## 1. Defining the task.

- The components needed to define a task are:
  - Data containing the predictor variables
  - The target variable we want to predict.
- In case of supervised learning the target variable will be categorical if we have a classification problem, and continuous if we have a regression problem.
- In case of unsupervised learning we omit the target variable from our task definition.

---

- To define a classification task, we use the `makeClassifTask()` function.

```{r}
#| code-fold: false
library(mlr)
makeClassifTask(data = diabetes, target = "class") -> diabetesTask
diabetesTask
```

## 2. Defining the learner.
- The following components are required to define a learner
  1. The class of algorithm we are using
    - *classif* for classification
    - *regr* for regression
    - *cluster* for clustering 
  2. The algorithm we are using.

---

- In the `diabetes` data example we shall use

```{r}
#| code-fold: false
makeLearner("classif.knn", par.vals = list("k" = 2))-> knn
knn
```

## 3. Training the model
- We use the `train()` function to train our model
```{r}
#| code-fold: false
(train(learner = knn, task = diabetesTask) -> knnModel)
```

---

- Now that we have our model, let's pass the data through it and see how it performs.
```{r}
#| code-fold: false
(predict(knnModel, newdata = diabetes)-> knnPred)
```

---

- In order to assess the performance of our model we shall use the `performance()` function.
- We also specify the *performance metrics* by supplying them as a list to the **measures** argument.
- The two measures are 
  - `mmce` the mean misclassification error, which is the proportion of cases classified as a case other than their true class.
  - `acc` the accuracy, i.e. the proportion of cases that were correctly classified by the model.
  - `mmce` and `acc` sum to 1.

---

```{r}
#| code-fold: false
performance(pred = knnPred, measures = list(mmce, acc))
```

- We can see that the model is correctly classifying `r round(performance(pred = knnPred, measures = list(mmce, acc))[[2]]*100, 2)` percentage of cases.
- However, we are not sure about the model performance on **unseen** data.
- Therefore, model performance should not be assessed in this way.

## Balancing two sources of model error: the bias-variance tradeoff.

- *Underfitting* and *overfitting* are two important sources of errors in model building.
- In case of *underfitting*, we may have included few predictors or too simple a model to adequately describe the relationships/patterns in data.
- The fitted model is *biased*, i.e. a mmodel that performs poorly on the data we use to train it and on new data.
- *Because we typically like to explain away as much variation in our data as possible, and because we often have many more variables than are important for our problem, underfitting is less frequently a problem than overfitting*.

---

- In case of *overfitting* we may include too many predictors or too complex a model.
- Here, we usually model the *noise* along with the patterns/relationships in the data.
- *Noise* in a dataset is variation that is not systematically related to variables we have measured, but rather is due to inherent variability and/or error in measurement of our variables.
- If we model the *noise*, our model may perform very well on the training data but not on new data.

---

- Underfitting and overfitting both introduce error and reduce the **generalizability** of the model: *the ability of the model to generalize to future, unseen data*. 
- They are also opposed to each other: somewhere between a model that underfits and has bias, and a model that overfits and has variance, is an optimal model that balances the bias-variance trade-off.

![](5.Classification_files/fig3.10.PNG)




